{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dce442bc",
   "metadata": {},
   "source": [
    "# Nuclei Segmentation Model Results Visualisation\n",
    "Compare model predictions against keypoint annotations.\n",
    "Visualize: Original Image, Cancer Mask, Ground Truth Keypoints, Predicted Masks\n",
    "Show best, average, and worst performing examples based on F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594a872c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import label as scipy_label\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['SM_FRAMEWORK'] = 'tf.keras'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, UpSampling2D, Add, Multiply, concatenate, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import EfficientNetB4\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "import segmentation_models as sm\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "BASE_DIR = Path('')\n",
    "DATA_ROOT = BASE_DIR / \"\"\n",
    "\n",
    "MODEL_PATH = BASE_DIR / ''\n",
    "MODEL_TYPE = 'unet'\n",
    "BACKBONE = 'seresnet50'\n",
    "\n",
    "TISSUE_MODEL_SPEC_PATH = BASE_DIR / ''\n",
    "TISSUE_BACKBONE = 'seresnet50'\n",
    "\n",
    "IMG_HEIGHT, IMG_WIDTH = 512, 512\n",
    "NUM_CLASSES = 4\n",
    "TISSUE_NUM_CLASSES = 3\n",
    "\n",
    "CLASS_NAMES = ['Background', 'Negative', 'Positive', 'Boundaries']\n",
    "CLASS_MAPPING = {\n",
    "    (255, 255, 255): 0, \n",
    "    (112, 112, 225): 1, \n",
    "    (250, 62, 62): 2,   \n",
    "    (0, 0, 0): 3,       \n",
    "}\n",
    "INVERSE_CLASS_MAPPING = {v: k for k, v in CLASS_MAPPING.items()}\n",
    "\n",
    "KEYPOINT_TO_CLASS = {\n",
    "    'negative': 1, \n",
    "    'positive': 2, \n",
    "}\n",
    "\n",
    "JSON_CATEGORY_ID_TO_NAME = {\n",
    "    0: 'negative',\n",
    "    1: 'positive'\n",
    "}\n",
    "\n",
    "CANCER_CLASS_ID = 1\n",
    "\n",
    "print(\" Imports and configuration loaded\")\n",
    "print(f\"  Base directory: {BASE_DIR}\")\n",
    "print(f\"  Data root: {DATA_ROOT}\")\n",
    "print(f\"  Model path: {MODEL_PATH}\")\n",
    "print(f\"  Model exists: {MODEL_PATH.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee838b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_mask_to_colors(mask):\n",
    "    \"\"\"Convert class mask to RGB color image\"\"\"\n",
    "    color_mask = np.zeros((mask.shape[0], mask.shape[1], 3), dtype=np.uint8)\n",
    "    for class_index, color in INVERSE_CLASS_MAPPING.items():\n",
    "        color_mask[mask == class_index] = color\n",
    "    return color_mask\n",
    "\n",
    "def find_connected_components(mask, class_id):\n",
    "    \"\"\"Find connected components for a specific class\"\"\"\n",
    "    binary_mask = (mask == class_id).astype(np.uint8)\n",
    "    labeled_mask, num_features = scipy_label(binary_mask)\n",
    "    components = []\n",
    "    for i in range(1, num_features + 1):\n",
    "        component_mask = (labeled_mask == i)\n",
    "        components.append({\n",
    "            'id': i,\n",
    "            'mask': component_mask,\n",
    "            'area': component_mask.sum(),\n",
    "            'centroid': np.array(np.where(component_mask)).mean(axis=1)[::-1]\n",
    "        })\n",
    "    return components\n",
    "\n",
    "def extract_cancer_region_ensemble(image, model_spec):\n",
    "    \"\"\"Extract cancer region using tissue specialist model\"\"\"\n",
    "    input_tensor = np.expand_dims(image, axis=0)\n",
    "    pred_spec = model_spec.predict(input_tensor, verbose=0)[0]\n",
    "    mask_spec = np.argmax(pred_spec, axis=-1)\n",
    "    cancer_mask = (mask_spec == CANCER_CLASS_ID).astype(np.uint8)\n",
    "    cancer_image = image.copy()\n",
    "    cancer_image[cancer_mask == 0] = 1.0\n",
    "    return cancer_mask, cancer_image\n",
    "\n",
    "def filter_keypoints_by_cancer_mask(keypoints, cancer_mask):\n",
    "    \"\"\"Filter keypoints: Keep only those inside the cancer mask\"\"\"\n",
    "    filtered_keypoints = []\n",
    "    excluded_keypoints = []\n",
    "    \n",
    "    for kp in keypoints:\n",
    "        x_int = int(round(kp['local_x']))\n",
    "        y_int = int(round(kp['local_y']))\n",
    "        \n",
    "        if 0 <= y_int < cancer_mask.shape[0] and 0 <= x_int < cancer_mask.shape[1]:\n",
    "            if cancer_mask[y_int, x_int] == 1:\n",
    "                filtered_keypoints.append(kp)\n",
    "            else:\n",
    "                excluded_keypoints.append(kp)\n",
    "        else:\n",
    "            excluded_keypoints.append(kp)\n",
    "    \n",
    "    return filtered_keypoints, excluded_keypoints\n",
    "\n",
    "def remove_boundaries_from_nuclei(nuclei_mask):\n",
    "    \"\"\"Remove boundary class (class 3) from nuclei mask\"\"\"\n",
    "    processed_mask = nuclei_mask.copy()\n",
    "    processed_mask[processed_mask == 3] = 0\n",
    "    return processed_mask\n",
    "\n",
    "def remove_small_objects_from_mask(nuclei_mask, min_size=50):\n",
    "    \"\"\"Remove small connected components\"\"\"\n",
    "    processed_mask = nuclei_mask.copy()\n",
    "    \n",
    "    for class_id in [1, 2]:  \n",
    "        binary_mask = (processed_mask == class_id).astype(np.uint8)\n",
    "        labeled_mask, num_objects = scipy_label(binary_mask)\n",
    "        \n",
    "        for obj_id in range(1, num_objects + 1):\n",
    "            obj_pixels = np.sum(labeled_mask == obj_id)\n",
    "            if obj_pixels < min_size:\n",
    "                processed_mask[labeled_mask == obj_id] = 0\n",
    "    \n",
    "    return processed_mask\n",
    "\n",
    "def apply_post_processing(pred_mask, min_object_size=50):\n",
    "    \"\"\"Apply full post-processing pipeline\"\"\"\n",
    "    mask_no_boundaries = remove_boundaries_from_nuclei(pred_mask)\n",
    "    processed_mask = remove_small_objects_from_mask(mask_no_boundaries, min_size=min_object_size)\n",
    "    return processed_mask\n",
    "\n",
    "def load_slide_data(data_root):\n",
    "    \"\"\"Load all data from test set\"\"\"\n",
    "    data_root = Path(data_root)\n",
    "    slide_dirs = [d for d in data_root.iterdir() if d.is_dir()]\n",
    "    \n",
    "    all_data = []\n",
    "    print(f\"Scanning {len(slide_dirs)} slide directories...\")\n",
    "    \n",
    "    for slide_dir in sorted(slide_dirs):\n",
    "        annotations_file = slide_dir / \"annotations.json\"\n",
    "        patches_dir = slide_dir / \"patches_512_30\"\n",
    "        \n",
    "        if not annotations_file.exists():\n",
    "            if (patches_dir / \"annotations.json\").exists():\n",
    "                annotations_file = patches_dir / \"annotations.json\"\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "        if not patches_dir.exists():\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            with open(annotations_file, 'r') as f:\n",
    "                coco_data = json.load(f)\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "            \n",
    "        img_id_to_kps = defaultdict(list)\n",
    "        for ann in coco_data.get('annotations', []):\n",
    "            img_id = ann['image_id']\n",
    "            cat_id = ann['category_id']\n",
    "            kp_x, kp_y = ann['keypoints'][0], ann['keypoints'][1]\n",
    "            label_name = JSON_CATEGORY_ID_TO_NAME.get(cat_id)\n",
    "            if label_name:\n",
    "                img_id_to_kps[img_id].append({\n",
    "                    'label': label_name,\n",
    "                    'local_x': kp_x,\n",
    "                    'local_y': kp_y\n",
    "                })\n",
    "        \n",
    "        for img_entry in coco_data.get('images', []):\n",
    "            img_id = img_entry['id']\n",
    "            file_name = Path(img_entry['file_name']).name\n",
    "            patch_path = patches_dir / file_name\n",
    "            keypoints = img_id_to_kps.get(img_id, [])\n",
    "            \n",
    "            if patch_path.exists() and len(keypoints) > 0:\n",
    "                all_data.append({\n",
    "                    'slide_dir': slide_dir,\n",
    "                    'patch_path': patch_path,\n",
    "                    'patch_info': {\n",
    "                        'filename': file_name,\n",
    "                        'keypoints': keypoints\n",
    "                    }\n",
    "                })\n",
    "\n",
    "    print(f\"Found {len(all_data)} valid patches.\")\n",
    "    return all_data\n",
    "\n",
    "print(\" Helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c98964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention U-Net architecture\n",
    "\n",
    "def attention_gate(gating_signal, skip_connection, inter_channels):\n",
    "    theta_g = Conv2D(inter_channels, kernel_size=1, strides=1, padding=\"same\")(gating_signal)\n",
    "    theta_g = BatchNormalization()(theta_g)\n",
    "    phi_x = Conv2D(inter_channels, kernel_size=1, strides=1, padding=\"same\")(skip_connection)\n",
    "    phi_x = BatchNormalization()(phi_x)\n",
    "    add_xg = Add()([theta_g, phi_x])\n",
    "    act_xg = Activation(\"relu\")(add_xg)\n",
    "    psi = Conv2D(1, kernel_size=1, strides=1, padding=\"same\")(act_xg)\n",
    "    psi = BatchNormalization()(psi)\n",
    "    psi = Activation(\"sigmoid\")(psi)\n",
    "    return Multiply()([skip_connection, psi])\n",
    "\n",
    "def conv_block(x, filters, kernel_size=3):\n",
    "    x = Conv2D(filters, kernel_size, padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = Conv2D(filters, kernel_size, padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    return Activation(\"relu\")(x)\n",
    "\n",
    "def decoder_block(x, skip, filters, use_attention=True, dropout_rate=0.1):\n",
    "    x = UpSampling2D(size=(2, 2))(x)\n",
    "    x = Conv2D(filters, kernel_size=2, padding=\"same\")(x)\n",
    "    if use_attention:\n",
    "        skip = attention_gate(gating_signal=x, skip_connection=skip, inter_channels=filters // 2)\n",
    "    x = concatenate([x, skip], axis=-1)\n",
    "    if dropout_rate > 0:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "    return conv_block(x, filters)\n",
    "\n",
    "def build_attention_unet(input_shape=(IMG_HEIGHT, IMG_WIDTH, 3), num_classes=NUM_CLASSES, backbone=\"efficientnetb4\"):\n",
    "    keras.backend.set_image_data_format(\"channels_last\")\n",
    "    input_layer = Input(shape=(512, 512, 3), name=\"input_layer\")\n",
    "    base_model = EfficientNetB4(include_top=False, weights=None, input_tensor=input_layer)\n",
    "    try:\n",
    "        weights_path = tf.keras.utils.get_file(\"efficientnetb4_notop.h5\", \"https://storage.googleapis.com/keras-applications/efficientnetb4_notop.h5\", cache_subdir=\"models\")\n",
    "        base_model.load_weights(weights_path, skip_mismatch=True, by_name=True)\n",
    "    except:\n",
    "        pass\n",
    "    skip_names = [\"block2a_expand_activation\", \"block3a_expand_activation\", \"block4a_expand_activation\", \"block6a_expand_activation\"]\n",
    "    skip_connections = [base_model.get_layer(name).output for name in skip_names]\n",
    "    encoder_model = Model(inputs=base_model.input, outputs=skip_connections + [base_model.output])\n",
    "    inputs = Input(shape=input_shape, name=\"input_layer\")\n",
    "    all_outputs = encoder_model(inputs, training=False)\n",
    "    skip_connections = all_outputs[:-1]\n",
    "    bottleneck = all_outputs[-1]\n",
    "    skip4, skip3, skip2, skip1 = skip_connections[3], skip_connections[2], skip_connections[1], skip_connections[0]\n",
    "    dec4 = decoder_block(bottleneck, skip4, 512, use_attention=True)\n",
    "    dec3 = decoder_block(dec4, skip3, 256, use_attention=True)\n",
    "    dec2 = decoder_block(dec3, skip2, 128, use_attention=True)\n",
    "    dec1 = decoder_block(dec2, skip1, 64, use_attention=True)\n",
    "    final_up = UpSampling2D(size=(2, 2))(dec1)\n",
    "    final_conv = Conv2D(64, kernel_size=3, padding=\"same\")(final_up)\n",
    "    final_conv = BatchNormalization()(final_conv)\n",
    "    final_conv = Activation(\"relu\")(final_conv)\n",
    "    outputs = Conv2D(num_classes, kernel_size=1, padding=\"same\", activation=\"softmax\")(final_conv)\n",
    "    return Model(inputs=inputs, outputs=outputs, name=f\"attention_unet_{backbone}\")\n",
    "\n",
    "print(\" Attention U-Net architecture defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e3d90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_load_nuclei_model():\n",
    "    \"\"\"Build and load nuclei segmentation model\"\"\"\n",
    "    print(f\"Initializing Nuclei Model: {MODEL_TYPE} with {BACKBONE}...\")\n",
    "    \n",
    "    if 'attention' in MODEL_TYPE.lower():\n",
    "        model = build_attention_unet(input_shape=(IMG_HEIGHT, IMG_WIDTH, 3), num_classes=NUM_CLASSES, backbone=BACKBONE)\n",
    "    else:\n",
    "        model = sm.Unet(backbone_name=BACKBONE, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3), classes=NUM_CLASSES, activation='softmax', encoder_weights=None)\n",
    "    \n",
    "    print(f\"Loading weights from: {MODEL_PATH}\")\n",
    "    try:\n",
    "        model.load_weights(str(MODEL_PATH))\n",
    "        print(\" Weights loaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Standard load failed ({e}). Trying with skip_mismatch=True...\")\n",
    "        model.load_weights(str(MODEL_PATH), skip_mismatch=True)\n",
    "        print(\" Weights loaded with skip_mismatch\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def load_tissue_model():\n",
    "    \"\"\"Load tissue specialist model\"\"\"\n",
    "    print(f\"Loading Tissue Specialist Model (Backbone: {TISSUE_BACKBONE})...\")\n",
    "    model_spec = sm.Unet(backbone_name=TISSUE_BACKBONE, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3), classes=TISSUE_NUM_CLASSES, activation='softmax', encoder_weights=None)\n",
    "    model_spec.load_weights(str(TISSUE_MODEL_SPEC_PATH))\n",
    "    print(\" Tissue model loaded\")\n",
    "    return model_spec\n",
    "\n",
    "print(\"Loading models...\")\n",
    "nuclei_model = build_and_load_nuclei_model()\n",
    "tissue_model = load_tissue_model()\n",
    "print(\" All models loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0dfaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def check_component_has_keypoint(component, keypoints, expected_class, proximity_threshold=3):\n",
    "    \"\"\"Check if a component has a keypoint inside it or within proximity_threshold pixels\"\"\"\n",
    "    component_mask = component['mask']\n",
    "    \n",
    "    for kp in keypoints:\n",
    "        if kp['label'] == expected_class:\n",
    "            x_int = int(round(kp['local_x']))\n",
    "            y_int = int(round(kp['local_y']))\n",
    "            \n",
    "            if 0 <= y_int < component_mask.shape[0] and 0 <= x_int < component_mask.shape[1]:\n",
    "                if component_mask[y_int, x_int]:\n",
    "                    return True\n",
    "                \n",
    "                y_min = max(0, y_int - proximity_threshold)\n",
    "                y_max = min(component_mask.shape[0], y_int + proximity_threshold + 1)\n",
    "                x_min = max(0, x_int - proximity_threshold)\n",
    "                x_max = min(component_mask.shape[1], x_int + proximity_threshold + 1)\n",
    "                \n",
    "                window = component_mask[y_min:y_max, x_min:x_max]\n",
    "                if np.any(window):\n",
    "                    return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def calculate_keypoint_coverage_f1(keypoints, pred_mask):\n",
    "    \"\"\"\n",
    "    Calculate F1-like score for keypoint coverage.\n",
    "    \n",
    "    Metrics:\n",
    "    - Precision: How many predicted nuclei contain keypoints (mask validity)\n",
    "    - Recall: How many keypoints are correctly covered\n",
    "    - F1: Harmonic mean\n",
    "    \"\"\"\n",
    "    if len(keypoints) == 0:\n",
    "        return 0.0, 0.0, 0.0, {}\n",
    "    \n",
    "    correct_keypoints = 0\n",
    "    for kp in keypoints:\n",
    "        label = kp['label']\n",
    "        expected_class_id = KEYPOINT_TO_CLASS[label]\n",
    "        x_int = int(round(kp['local_x']))\n",
    "        y_int = int(round(kp['local_y']))\n",
    "        \n",
    "        if 0 <= x_int < pred_mask.shape[1] and 0 <= y_int < pred_mask.shape[0]:\n",
    "            if pred_mask[y_int, x_int] == expected_class_id:\n",
    "                correct_keypoints += 1\n",
    "    \n",
    "    recall = correct_keypoints / len(keypoints) if len(keypoints) > 0 else 0.0\n",
    "    \n",
    "    total_predicted_nuclei = 0\n",
    "    valid_predicted_nuclei = 0\n",
    "    \n",
    "    for class_name, class_id in [('negative', 1), ('positive', 2)]:\n",
    "        components = find_connected_components(pred_mask, class_id)\n",
    "        total_predicted_nuclei += len(components)\n",
    "        \n",
    "        for component in components:\n",
    "            if check_component_has_keypoint(component, keypoints, class_name):\n",
    "                valid_predicted_nuclei += 1\n",
    "    \n",
    "    precision = valid_predicted_nuclei / total_predicted_nuclei if total_predicted_nuclei > 0 else 0.0\n",
    "    \n",
    "    if precision + recall > 0:\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        f1 = 0.0\n",
    "    \n",
    "    details = {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'correct_keypoints': correct_keypoints,\n",
    "        'total_keypoints': len(keypoints),\n",
    "        'valid_predicted_nuclei': valid_predicted_nuclei,\n",
    "        'total_predicted_nuclei': total_predicted_nuclei\n",
    "    }\n",
    "    \n",
    "    return f1, precision, recall, details\n",
    "\n",
    "\n",
    "def evaluate_patch_with_metrics_extended(image_path, keypoints, nuclei_model, tissue_model):\n",
    "    \"\"\"Evaluate a single patch and return metrics INCLUDING raw prediction\"\"\"\n",
    "    img_raw = img_to_array(load_img(str(image_path), target_size=(512, 512)))\n",
    "    img_norm = img_raw / 255.0\n",
    "    \n",
    "    cancer_mask, cancer_image = extract_cancer_region_ensemble(img_norm, tissue_model)\n",
    "    \n",
    "    filtered_kps, excluded_kps = filter_keypoints_by_cancer_mask(keypoints, cancer_mask)\n",
    "    \n",
    "    if 'efficientnet' in BACKBONE:\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        inp = (cancer_image - mean) / std\n",
    "    else:\n",
    "        inp = cancer_image\n",
    "    \n",
    "    pred = nuclei_model.predict(np.expand_dims(inp, axis=0), verbose=0)[0]\n",
    "    pred_mask_raw = np.argmax(pred, axis=-1)\n",
    "    \n",
    "    pred_mask_processed = apply_post_processing(pred_mask_raw, min_object_size=50)\n",
    "    \n",
    "    f1, precision, recall, details = calculate_keypoint_coverage_f1(filtered_kps, pred_mask_processed)\n",
    "    \n",
    "    return {\n",
    "        'image': img_raw.astype(np.uint8),\n",
    "        'cancer_mask': cancer_mask,\n",
    "        'pred_mask_raw': pred_mask_raw,\n",
    "        'pred_mask': pred_mask_processed,\n",
    "        'keypoints': filtered_kps,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'details': details\n",
    "    }\n",
    "\n",
    "print(\" Metrics functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb184b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def calculate_spatial_correlation_metrics(keypoints, pred_mask):\n",
    "    if len(keypoints) == 0:\n",
    "        return None\n",
    "    \n",
    "    gt_negative_count = sum(1 for kp in keypoints if kp['label'] == 'negative')\n",
    "    gt_positive_count = sum(1 for kp in keypoints if kp['label'] == 'positive')\n",
    "    \n",
    "    neg_components = find_connected_components(pred_mask, 1)\n",
    "    pos_components = find_connected_components(pred_mask, 2)\n",
    "    pred_negative_count = len(neg_components)\n",
    "    pred_positive_count = len(pos_components)\n",
    "    \n",
    "    spatial_metrics = {'negative': [], 'positive': []}\n",
    "    \n",
    "    for class_name, class_id, components in [\n",
    "        ('negative', 1, neg_components),\n",
    "        ('positive', 2, pos_components)\n",
    "    ]:\n",
    "        class_keypoints = [kp for kp in keypoints if kp['label'] == class_name]\n",
    "        \n",
    "        if len(class_keypoints) > 0 and len(components) > 0:\n",
    "            kp_coords = np.array([[kp['local_x'], kp['local_y']] for kp in class_keypoints])\n",
    "            \n",
    "            centroids = np.array([comp['centroid'] for comp in components])\n",
    "            \n",
    "            distances = cdist(kp_coords, centroids, metric='euclidean')\n",
    "            \n",
    "            min_distances = distances.min(axis=1)\n",
    "            spatial_metrics[class_name] = min_distances.tolist()\n",
    "    \n",
    "    gt_ratio = gt_positive_count / max(gt_negative_count, 1)\n",
    "    pred_ratio = pred_positive_count / max(pred_negative_count, 1)\n",
    "    \n",
    "    return {\n",
    "        'count_metrics': {\n",
    "            'gt_negative': gt_negative_count,\n",
    "            'gt_positive': gt_positive_count,\n",
    "            'pred_negative': pred_negative_count,\n",
    "            'pred_positive': pred_positive_count,\n",
    "            'gt_total': gt_negative_count + gt_positive_count,\n",
    "            'pred_total': pred_negative_count + pred_positive_count\n",
    "        },\n",
    "        'spatial_metrics': {\n",
    "            'negative_distances': spatial_metrics['negative'],\n",
    "            'positive_distances': spatial_metrics['positive'],\n",
    "            'negative_mean_dist': np.mean(spatial_metrics['negative']) if spatial_metrics['negative'] else None,\n",
    "            'negative_median_dist': np.median(spatial_metrics['negative']) if spatial_metrics['negative'] else None,\n",
    "            'positive_mean_dist': np.mean(spatial_metrics['positive']) if spatial_metrics['positive'] else None,\n",
    "            'positive_median_dist': np.median(spatial_metrics['positive']) if spatial_metrics['positive'] else None,\n",
    "        },\n",
    "        'ratio_metrics': {\n",
    "            'gt_pos_neg_ratio': gt_ratio,\n",
    "            'pred_pos_neg_ratio': pred_ratio,\n",
    "            'ratio_difference': abs(gt_ratio - pred_ratio)\n",
    "        }\n",
    "    }\n",
    "\n",
    "def aggregate_correlation_metrics(all_patch_metrics):\n",
    "    \"\"\"\n",
    "    Aggregate correlation metrics across all patches.\n",
    "    Calculate overall Pearson correlation for counts.\n",
    "    \"\"\"\n",
    "    if not all_patch_metrics:\n",
    "        return None\n",
    "    \n",
    "    gt_neg_counts = [m['count_metrics']['gt_negative'] for m in all_patch_metrics]\n",
    "    gt_pos_counts = [m['count_metrics']['gt_positive'] for m in all_patch_metrics]\n",
    "    gt_total_counts = [m['count_metrics']['gt_total'] for m in all_patch_metrics]\n",
    "    \n",
    "    pred_neg_counts = [m['count_metrics']['pred_negative'] for m in all_patch_metrics]\n",
    "    pred_pos_counts = [m['count_metrics']['pred_positive'] for m in all_patch_metrics]\n",
    "    pred_total_counts = [m['count_metrics']['pred_total'] for m in all_patch_metrics]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    if len(gt_neg_counts) > 1:\n",
    "        pearson_neg, p_pearson_neg = pearsonr(gt_neg_counts, pred_neg_counts)\n",
    "        results['negative'] = {\n",
    "            'pearson': pearson_neg,\n",
    "            'pearson_p': p_pearson_neg\n",
    "        }\n",
    "    \n",
    "    if len(gt_pos_counts) > 1:\n",
    "        pearson_pos, p_pearson_pos = pearsonr(gt_pos_counts, pred_pos_counts)\n",
    "        results['positive'] = {\n",
    "            'pearson': pearson_pos,\n",
    "            'pearson_p': p_pearson_pos\n",
    "        }\n",
    "    \n",
    "    if len(gt_total_counts) > 1:\n",
    "        pearson_total, p_pearson_total = pearsonr(gt_total_counts, pred_total_counts)\n",
    "        results['total'] = {\n",
    "            'pearson': pearson_total,\n",
    "            'pearson_p': p_pearson_total\n",
    "        }\n",
    "    \n",
    "    all_neg_distances = [d for m in all_patch_metrics for d in m['spatial_metrics']['negative_distances']]\n",
    "    all_pos_distances = [d for m in all_patch_metrics for d in m['spatial_metrics']['positive_distances']]\n",
    "    \n",
    "    results['spatial_aggregated'] = {\n",
    "        'negative_mean_dist': np.mean(all_neg_distances) if all_neg_distances else None,\n",
    "        'negative_median_dist': np.median(all_neg_distances) if all_neg_distances else None,\n",
    "        'positive_mean_dist': np.mean(all_pos_distances) if all_pos_distances else None,\n",
    "        'positive_median_dist': np.median(all_pos_distances) if all_pos_distances else None,\n",
    "    }\n",
    "    \n",
    "    results['raw_data'] = {\n",
    "        'gt_negative': gt_neg_counts,\n",
    "        'gt_positive': gt_pos_counts,\n",
    "        'gt_total': gt_total_counts,\n",
    "        'pred_negative': pred_neg_counts,\n",
    "        'pred_positive': pred_pos_counts,\n",
    "        'pred_total': pred_total_counts,\n",
    "        'negative_distances': all_neg_distances,\n",
    "        'positive_distances': all_pos_distances\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\" Correlation metrics functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b59d603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_count_correlation(correlation_results, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot scatter plots showing correlation between GT and predicted nuclei counts.\n",
    "    Creates 3 subplots: Negative, Positive, and Total nuclei counts.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    raw_data = correlation_results['raw_data']\n",
    "    \n",
    "    ax = axes[0]\n",
    "    ax.scatter(raw_data['gt_negative'], raw_data['pred_negative'], alpha=0.6, s=50, edgecolors='k', linewidth=0.5)\n",
    "    \n",
    "    max_val = max(max(raw_data['gt_negative']), max(raw_data['pred_negative']))\n",
    "    ax.plot([0, max_val], [0, max_val], 'r--', linewidth=2, label='Perfect correlation')\n",
    "    \n",
    "    z = np.polyfit(raw_data['gt_negative'], raw_data['pred_negative'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    ax.plot(raw_data['gt_negative'], p(raw_data['gt_negative']), 'b-', linewidth=2, alpha=0.7, label='Best fit')\n",
    "    \n",
    "    ax.set_xlabel('Ground Truth Count', fontsize=12)\n",
    "    ax.set_ylabel('Predicted Count', fontsize=12)\n",
    "    ax.set_title(f\"Negative Nuclei Count Correlation\\nPearson r={correlation_results['negative']['pearson']:.3f} (p={correlation_results['negative']['pearson_p']:.4f})\", fontsize=11)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax = axes[1]\n",
    "    ax.scatter(raw_data['gt_positive'], raw_data['pred_positive'], alpha=0.6, s=50, color='red', edgecolors='k', linewidth=0.5)\n",
    "    \n",
    "    max_val = max(max(raw_data['gt_positive']), max(raw_data['pred_positive']))\n",
    "    ax.plot([0, max_val], [0, max_val], 'darkred', linestyle='--', linewidth=2, label='Perfect correlation')\n",
    "    \n",
    "    z = np.polyfit(raw_data['gt_positive'], raw_data['pred_positive'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    ax.plot(raw_data['gt_positive'], p(raw_data['gt_positive']), 'orange', linewidth=2, alpha=0.7, label='Best fit')\n",
    "    \n",
    "    ax.set_xlabel('Ground Truth Count', fontsize=12)\n",
    "    ax.set_ylabel('Predicted Count', fontsize=12)\n",
    "    ax.set_title(f\"Positive Nuclei Count Correlation\\nPearson r={correlation_results['positive']['pearson']:.3f} (p={correlation_results['positive']['pearson_p']:.4f})\", fontsize=11)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax = axes[2]\n",
    "    ax.scatter(raw_data['gt_total'], raw_data['pred_total'], alpha=0.6, s=50, color='green', edgecolors='k', linewidth=0.5)\n",
    "    \n",
    "    max_val = max(max(raw_data['gt_total']), max(raw_data['pred_total']))\n",
    "    ax.plot([0, max_val], [0, max_val], 'darkgreen', linestyle='--', linewidth=2, label='Perfect correlation')\n",
    "    \n",
    "    z = np.polyfit(raw_data['gt_total'], raw_data['pred_total'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    ax.plot(raw_data['gt_total'], p(raw_data['gt_total']), 'lime', linewidth=2, alpha=0.7, label='Best fit')\n",
    "    \n",
    "    ax.set_xlabel('Ground Truth Count', fontsize=12)\n",
    "    ax.set_ylabel('Predicted Count', fontsize=12)\n",
    "    ax.set_title(f\"Total Nuclei Count Correlation\\nPearson r={correlation_results['total']['pearson']:.3f} (p={correlation_results['total']['pearson_p']:.4f})\", fontsize=11)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_spatial_distance_distribution(correlation_results, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot histograms showing distribution of distances from keypoints to nearest predicted nucleus.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    raw_data = correlation_results['raw_data']\n",
    "    spatial_agg = correlation_results['spatial_aggregated']\n",
    "    \n",
    "    ax = axes[0]\n",
    "    if raw_data['negative_distances']:\n",
    "        ax.hist(raw_data['negative_distances'], bins=30, color='blue', alpha=0.7, edgecolor='black')\n",
    "        ax.axvline(spatial_agg['negative_mean_dist'], color='red', linestyle='--', linewidth=2, label=f\"Mean: {spatial_agg['negative_mean_dist']:.2f}px\")\n",
    "        ax.axvline(spatial_agg['negative_median_dist'], color='orange', linestyle='--', linewidth=2, label=f\"Median: {spatial_agg['negative_median_dist']:.2f}px\")\n",
    "        ax.set_xlabel('Distance (pixels)', fontsize=12)\n",
    "        ax.set_ylabel('Frequency', fontsize=12)\n",
    "        ax.set_title(f\"Negative Nuclei: Distance from Keypoint to Nearest Prediction\\n(n={len(raw_data['negative_distances'])} keypoints)\", fontsize=11)\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'No negative keypoints', ha='center', va='center', transform=ax.transAxes, fontsize=14)\n",
    "        ax.set_title(\"Negative Nuclei: No Data\", fontsize=11)\n",
    "    \n",
    "    ax = axes[1]\n",
    "    if raw_data['positive_distances']:\n",
    "        ax.hist(raw_data['positive_distances'], bins=30, color='red', alpha=0.7, edgecolor='black')\n",
    "        ax.axvline(spatial_agg['positive_mean_dist'], color='darkred', linestyle='--', linewidth=2, label=f\"Mean: {spatial_agg['positive_mean_dist']:.2f}px\")\n",
    "        ax.axvline(spatial_agg['positive_median_dist'], color='orange', linestyle='--', linewidth=2, label=f\"Median: {spatial_agg['positive_median_dist']:.2f}px\")\n",
    "        ax.set_xlabel('Distance (pixels)', fontsize=12)\n",
    "        ax.set_ylabel('Frequency', fontsize=12)\n",
    "        ax.set_title(f\"Positive Nuclei: Distance from Keypoint to Nearest Prediction\\n(n={len(raw_data['positive_distances'])} keypoints)\", fontsize=11)\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'No positive keypoints', ha='center', va='center', transform=ax.transAxes, fontsize=14)\n",
    "        ax.set_title(\"Positive Nuclei: No Data\", fontsize=11)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def print_correlation_summary(correlation_results):\n",
    "    \"\"\"Print comprehensive correlation metrics summary.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"CORRELATION METRICS SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\nCount Correlation (Pearson):\")\n",
    "    print(\"-\" * 80)\n",
    "    for nuclei_type in ['negative', 'positive', 'total']:\n",
    "        if nuclei_type in correlation_results:\n",
    "            metrics = correlation_results[nuclei_type]\n",
    "            print(f\"\\n  {nuclei_type.upper()} Nuclei:\")\n",
    "            print(f\"    Pearson correlation:  r = {metrics['pearson']:.4f}  (p-value: {metrics['pearson_p']:.4e})\")\n",
    "            \n",
    "            r_val = abs(metrics['pearson'])\n",
    "            if r_val >= 0.9:\n",
    "                interp = \"Very strong\"\n",
    "            elif r_val >= 0.7:\n",
    "                interp = \"Strong\"\n",
    "            elif r_val >= 0.5:\n",
    "                interp = \"Moderate\"\n",
    "            elif r_val >= 0.3:\n",
    "                interp = \"Weak\"\n",
    "            else:\n",
    "                interp = \"Very weak\"\n",
    "            print(f\"    Interpretation: {interp} correlation\")\n",
    "    \n",
    "    print(\"\\n\\nSpatial Distance Metrics (Keypoint to Nearest Prediction):\")\n",
    "    print(\"-\" * 80)\n",
    "    spatial = correlation_results['spatial_aggregated']\n",
    "    \n",
    "    if spatial['negative_mean_dist'] is not None:\n",
    "        print(f\"\\n  NEGATIVE Nuclei:\")\n",
    "        print(f\"    Mean distance:   {spatial['negative_mean_dist']:.2f} pixels\")\n",
    "        print(f\"    Median distance: {spatial['negative_median_dist']:.2f} pixels\")\n",
    "    \n",
    "    if spatial['positive_mean_dist'] is not None:\n",
    "        print(f\"\\n  POSITIVE Nuclei:\")\n",
    "        print(f\"    Mean distance:   {spatial['positive_mean_dist']:.2f} pixels\")\n",
    "        print(f\"    Median distance: {spatial['positive_median_dist']:.2f} pixels\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "print(\" Correlation visualization functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83a8f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_prediction_with_filtering(result, title_prefix=\"\"):\n",
    "    \"\"\"\n",
    "    Visualize prediction with 4 panels showing post-processing filtering:\n",
    "    1. Original Image (input)\n",
    "    2. Ground Truth (Cancer Region + GT Keypoints)\n",
    "    3. Prediction with Filtering Cues (Green=Accepted, Red=Rejected)\n",
    "    4. Final Clean Result vs GT (Error Analysis)\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "    \n",
    "    img = result['image']\n",
    "    cancer_mask = result['cancer_mask']\n",
    "    pred_mask_raw = result.get('pred_mask_raw', result['pred_mask'])  \n",
    "    pred_mask_final = result['pred_mask']  \n",
    "    keypoints = result['keypoints']\n",
    "    f1 = result['f1']\n",
    "    precision = result['precision']\n",
    "    recall = result['recall']\n",
    "    \n",
    "    axes[0].imshow(img)\n",
    "    axes[0].set_title(\"Input Image\", fontsize=11, fontweight='bold')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    overlay = np.zeros_like(img)\n",
    "    overlay[cancer_mask == 1] = [255, 255, 0]\n",
    "    panel2 = img.copy()\n",
    "    mask_indices = cancer_mask == 1\n",
    "    \n",
    "    if np.any(mask_indices):\n",
    "        alpha = 0.3\n",
    "        for c in range(3): \n",
    "            panel2[:, :, c][mask_indices] = (\n",
    "                img[:, :, c][mask_indices] * (1-alpha) + \n",
    "                overlay[:, :, c][mask_indices] * alpha\n",
    "            ).astype(np.uint8)\n",
    "    \n",
    "    axes[1].imshow(panel2)\n",
    "    for kp in keypoints:\n",
    "        color = 'red' if kp['label'] == 'positive' else 'blue'\n",
    "        axes[1].plot(kp['local_x'], kp['local_y'], 'o', color=color, markersize=5, \n",
    "                    markeredgecolor='white', markeredgewidth=0.8)\n",
    "    \n",
    "    gt_pos = sum(1 for kp in keypoints if kp['label'] == 'positive')\n",
    "    gt_neg = sum(1 for kp in keypoints if kp['label'] == 'negative')\n",
    "    axes[1].set_title(f\"Ground Truth\\nCancer ROI + Keypoints (Pos={gt_pos}, Neg={gt_neg})\", \n",
    "                     fontsize=11, fontweight='bold')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    panel3 = img.copy()\n",
    "    \n",
    "    removed_mask = (pred_mask_raw = 0) & (pred_mask_final == 0)\n",
    "    \n",
    "    accepted_neg = (pred_mask_final == 1)  \n",
    "    accepted_pos = (pred_mask_final == 2)  \n",
    "    \n",
    "    rejected_objects = removed_mask\n",
    "    \n",
    "    overlay3 = np.zeros_like(img)\n",
    "    overlay3[accepted_neg] = [0, 255, 0]      \n",
    "    overlay3[accepted_pos] = [0, 200, 0]      \n",
    "    overlay3[rejected_objects] = [255, 100, 0]  \n",
    "    \n",
    "    alpha = 0.5\n",
    "    panel3 = cv2.addWeighted(img, 1-alpha, overlay3, alpha, 0)\n",
    "    \n",
    "    axes[2].imshow(panel3)\n",
    "    \n",
    "    rejected_count = np.sum(removed_mask > 0)\n",
    "    accepted_count_neg = len(find_connected_components(pred_mask_final, 1))\n",
    "    accepted_count_pos = len(find_connected_components(pred_mask_final, 2))\n",
    "    \n",
    "    axes[2].set_title(f\"Prediction with Filtering\\n Accepted: {accepted_count_neg + accepted_count_pos} |  Rejected: {rejected_count} px\", \n",
    "                     fontsize=11, fontweight='bold')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    panel4 = decode_mask_to_colors(pred_mask_final)\n",
    "    \n",
    "    for kp in keypoints:\n",
    "        label = kp['label']\n",
    "        expected_class_id = KEYPOINT_TO_CLASS[label]\n",
    "        x_int = int(round(kp['local_x']))\n",
    "        y_int = int(round(kp['local_y']))\n",
    "        \n",
    "        if 0 <= x_int < pred_mask_final.shape[1] and 0 <= y_int < pred_mask_final.shape[0]:\n",
    "            is_correct = pred_mask_final[y_int, x_int] == expected_class_id\n",
    "            \n",
    "            if is_correct:\n",
    "                marker = 'o'\n",
    "                color = 'lime'\n",
    "                markersize = 6\n",
    "            else:\n",
    "                marker = 'x'\n",
    "                color = 'red'\n",
    "                markersize = 8\n",
    "            \n",
    "            axes[3].imshow(panel4)\n",
    "            axes[3].plot(kp['local_x'], kp['local_y'], marker, color=color, \n",
    "                        markersize=markersize, markeredgecolor='white', \n",
    "                        markeredgewidth=1.2, alpha=0.9)\n",
    "    \n",
    "    total_pred_nuclei = accepted_count_neg + accepted_count_pos\n",
    "    correctly_covered = result['details']['correct_keypoints']\n",
    "    false_negatives = len(keypoints) - correctly_covered\n",
    "    false_positives = total_pred_nuclei - result['details']['valid_predicted_nuclei']\n",
    "    \n",
    "    axes[3].set_title(f\"Final Result - Error Analysis\\nF1={f1:.3f} | FP={false_positives} | FN={false_negatives}\", \n",
    "                     fontsize=11, fontweight='bold')\n",
    "    axes[3].axis('off')\n",
    "    \n",
    "    if title_prefix:\n",
    "        fig.suptitle(title_prefix, fontsize=14, fontweight='bold', y=0.98)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(\" Visualization function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9899c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import heapq\n",
    "\n",
    "def evaluate_and_select_examples(min_keypoints=20, n_candidates=10, collect_correlation_metrics=False):\n",
    "    \"\"\"\n",
    "    Memory-efficient evaluation: process patches sequentially and track only\n",
    "    top candidates for visualization (best, average, worst with many keypoints).\n",
    "    \n",
    "    Args:\n",
    "        min_keypoints: Minimum keypoints required to consider a patch\n",
    "        n_candidates: How many candidates to track per category\n",
    "        collect_correlation_metrics: If True, collect correlation metrics for all patches\n",
    "    \"\"\"\n",
    "    print(\"Loading test data...\")\n",
    "    data_list = load_slide_data(DATA_ROOT)\n",
    "    \n",
    "    best_candidates = []  \n",
    "    worst_candidates = [] \n",
    "    \n",
    "    all_f1_scores = []\n",
    "    patch_metadata = []  \n",
    " \n",
    "    correlation_metrics_list = [] if collect_correlation_metrics else None\n",
    "    \n",
    "    print(f\"\\nPhase 1: Scanning {len(data_list)} patches for F1 distribution...\")\n",
    "    \n",
    "    for i, data in enumerate(data_list):\n",
    "        if (i+1) % 50 == 0:\n",
    "            print(f\"  Progress: {i+1}/{len(data_list)} patches processed\")\n",
    "        \n",
    "        try:\n",
    "            img_raw = img_to_array(load_img(str(data['patch_path']), target_size=(512, 512)))\n",
    "            img_norm = img_raw / 255.0\n",
    "            cancer_mask, cancer_image = extract_cancer_region_ensemble(img_norm, tissue_model)\n",
    "            filtered_kps, _ = filter_keypoints_by_cancer_mask(data['patch_info']['keypoints'], cancer_mask)\n",
    "            \n",
    "            if len(filtered_kps) < min_keypoints:\n",
    "                continue\n",
    "            \n",
    "            # Quick prediction to get F1\n",
    "            if 'efficientnet' in BACKBONE:\n",
    "                mean = np.array([0.485, 0.456, 0.406])\n",
    "                std = np.array([0.229, 0.224, 0.225])\n",
    "                inp = (cancer_image - mean) / std\n",
    "            else:\n",
    "                inp = cancer_image\n",
    "            \n",
    "            pred = nuclei_model.predict(np.expand_dims(inp, axis=0), verbose=0)[0]\n",
    "            pred_mask = np.argmax(pred, axis=-1)\n",
    "            pred_mask_processed = apply_post_processing(pred_mask, min_object_size=50)\n",
    "            \n",
    "            f1, precision, recall, details = calculate_keypoint_coverage_f1(filtered_kps, pred_mask_processed)\n",
    "            \n",
    "            if collect_correlation_metrics:\n",
    "                corr_metrics = calculate_spatial_correlation_metrics(filtered_kps, pred_mask_processed)\n",
    "                if corr_metrics:\n",
    "                    correlation_metrics_list.append(corr_metrics)\n",
    "            \n",
    "            patch_metadata.append({\n",
    "                'f1': f1,\n",
    "                'patch_path': data['patch_path'],\n",
    "                'keypoints': data['patch_info']['keypoints'],\n",
    "                'num_keypoints': len(filtered_kps),\n",
    "                'filename': data['patch_info']['filename'],\n",
    "                'slide_dir': data['slide_dir'].name\n",
    "            })\n",
    "            all_f1_scores.append(f1)\n",
    "            \n",
    "            if len(best_candidates) < n_candidates:\n",
    "                heapq.heappush(best_candidates, (f1, len(best_candidates), data['patch_path'], data['patch_info']))  # min-heap\n",
    "            elif f1 > best_candidates[0][0]:\n",
    "                heapq.heapreplace(best_candidates, (f1, len(best_candidates), data['patch_path'], data['patch_info']))\n",
    "            \n",
    "            if len(worst_candidates) < n_candidates:\n",
    "                heapq.heappush(worst_candidates, (-f1, len(worst_candidates), data['patch_path'], data['patch_info']))  # max-heap\n",
    "            elif f1 < -worst_candidates[0][0]:\n",
    "                heapq.heapreplace(worst_candidates, (-f1, len(worst_candidates), data['patch_path'], data['patch_info']))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error processing patch: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n Phase 1 complete: {len(patch_metadata)} patches with >={min_keypoints} keypoints found\")\n",
    "    \n",
    "    if len(all_f1_scores) == 0:\n",
    "        print(\"No valid patches found\")\n",
    "        return None, None, None\n",
    "    \n",
    "    median_f1 = np.median(all_f1_scores)\n",
    "    print(f\"\\nF1 Score distribution:\")\n",
    "    print(f\"  Mean: {np.mean(all_f1_scores):.4f} ± {np.std(all_f1_scores):.4f}\")\n",
    "    print(f\"  Median: {median_f1:.4f}\")\n",
    "    print(f\"  Range: [{np.min(all_f1_scores):.4f}, {np.max(all_f1_scores):.4f}]\")\n",
    "    \n",
    "    median_tolerance = 0.05\n",
    "    average_candidates = []\n",
    "    for meta in patch_metadata:\n",
    "        if abs(meta['f1'] - median_f1) <= median_tolerance:\n",
    "            average_candidates.append(meta)\n",
    "    \n",
    "    average_candidates.sort(key=lambda x: x['num_keypoints'], reverse=True)\n",
    "    average_candidates = average_candidates[:n_candidates]\n",
    "    \n",
    "    print(f\"\\nFound candidates:\")\n",
    "    print(f\"  Best: {len(best_candidates)} (F1 range: {min(x[0] for x in best_candidates):.4f} - {max(x[0] for x in best_candidates):.4f})\")\n",
    "    print(f\"  Average: {len(average_candidates)} (F1 around {median_f1:.4f} ± {median_tolerance})\")\n",
    "    print(f\"  Worst: {len(worst_candidates)} (F1 range: {min(-x[0] for x in worst_candidates):.4f} - {max(-x[0] for x in worst_candidates):.4f})\")\n",
    "    \n",
    "    if collect_correlation_metrics:\n",
    "        return best_candidates, average_candidates, worst_candidates, correlation_metrics_list\n",
    "    else:\n",
    "        return best_candidates, average_candidates, worst_candidates\n",
    "\n",
    "def visualize_selected_examples_with_filtering(best_candidates, average_candidates, worst_candidates, n_show=3):\n",
    "    \"\"\"\n",
    "    Visualize pre-selected examples with filtering visualization.\n",
    "    \"\"\"\n",
    "    \n",
    "    def evaluate_and_visualize(path, keypoints, title_prefix, f1_expected):\n",
    "        \"\"\"Helper to evaluate a specific patch and visualize it with filtering\"\"\"\n",
    "        result = evaluate_patch_with_metrics_extended(path, keypoints, nuclei_model, tissue_model)\n",
    "        print(f\"\\n{title_prefix}\")\n",
    "        print(f\"  F1: {result['f1']:.4f} | Precision: {result['precision']:.4f} | Recall: {result['recall']:.4f}\")\n",
    "        print(f\"  Keypoints: {len(result['keypoints'])}\")\n",
    "        visualize_prediction_with_filtering(result, title_prefix)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"TOP {n_show} BEST PERFORMING PATCHES (High F1, Many Keypoints)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    best_sorted = sorted(best_candidates, key=lambda x: x[0], reverse=True)[:n_show]\n",
    "    for i, (f1, _, path, patch_info) in enumerate(best_sorted, 1):\n",
    "        title = f\"Best #{i} | Expected F1: {f1:.4f}\"\n",
    "        evaluate_and_visualize(path, patch_info['keypoints'], title, f1)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"{n_show} AVERAGE PERFORMING PATCHES (Median F1, Many Keypoints)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    average_to_show = average_candidates[:n_show]\n",
    "    for i, meta in enumerate(average_to_show, 1):\n",
    "        title = f\"Average #{i} | Expected F1: {meta['f1']:.4f}\"\n",
    "        evaluate_and_visualize(meta['patch_path'], meta['keypoints'], title, meta['f1'])\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"TOP {n_show} WORST PERFORMING PATCHES (Low F1, Many Keypoints)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    worst_sorted = sorted(worst_candidates, key=lambda x: x[0], reverse=True)[:n_show]\n",
    "    for i, (neg_f1, _, path, patch_info) in enumerate(worst_sorted, 1):\n",
    "        f1 = -neg_f1\n",
    "        title = f\"Worst #{i} | Expected F1: {f1:.4f}\"\n",
    "        evaluate_and_visualize(path, patch_info['keypoints'], title, f1)\n",
    "\n",
    "\n",
    "print(\" Evaluation and showcase functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481a2949",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting memory-efficient evaluation...\")\n",
    "print(\"This will find best/average/worst patches with many keypoints (≥20)\")\n",
    "print(\"\\nNote: Only candidate patches are tracked, not all results stored in memory.\\n\")\n",
    "\n",
    "results = evaluate_and_select_examples(\n",
    "    min_keypoints=100,  # Only consider patches with ≥100 keypoints\n",
    "    n_candidates=10,    # Track top 10 candidates per category\n",
    "    collect_correlation_metrics=False\n",
    ")\n",
    "\n",
    "best_candidates, average_candidates, worst_candidates = results\n",
    "\n",
    "if best_candidates is not None:\n",
    "    visualize_selected_examples_with_filtering(\n",
    "        best_candidates, \n",
    "        average_candidates, \n",
    "        worst_candidates, \n",
    "        n_show=3\n",
    "    )\n",
    "    print(\"\\n Visualization complete\")\n",
    "else:\n",
    "    print(\"\\n✗ No valid patches found. Try lowering min_keypoints threshold.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fd91d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run correlation analysis on test set\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\" CORRELATION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nEvaluating all patches to calculate correlation metrics...\")\n",
    "print(\"This analyzes the relationship between keypoint annotations and model predictions.\\n\")\n",
    "\n",
    "results = evaluate_and_select_examples(\n",
    "    min_keypoints=20,  # Include patches with ≥20 keypoints\n",
    "    n_candidates=10,\n",
    "    collect_correlation_metrics=True\n",
    ")\n",
    "\n",
    "if len(results) == 4:\n",
    "    best_candidates, average_candidates, worst_candidates, correlation_metrics_list = results\n",
    "    \n",
    "    print(f\"\\n Collected correlation metrics from {len(correlation_metrics_list)} patches\")\n",
    "    print(\"Aggregating results...\")\n",
    "    \n",
    "    correlation_results = aggregate_correlation_metrics(correlation_metrics_list)\n",
    "    \n",
    "    print_correlation_summary(correlation_results)\n",
    "    \n",
    "    print(\"\\n\\n Generating correlation plots...\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    print(\"\\nCount Correlation Plots (Scatter plots with best fit lines)\")\n",
    "    plot_count_correlation(correlation_results)\n",
    "    \n",
    "    \n",
    "    print(\"\\nSpatial Distance Distribution (Keypoint to Nearest Prediction)\")\n",
    "    plot_spatial_distance_distribution(correlation_results)\n",
    "    \n",
    "    print(\"\\n\\n Correlation analysis complete\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "else:\n",
    "    print(\"Correlation metrics were not collected. Re-run with collect_correlation_metrics=True\")\n",
    "    best_candidates, average_candidates, worst_candidates = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89e4e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 Score Distribution Analysis\n",
    "\n",
    "def plot_f1_distribution(patch_metadata, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot histogram showing F1 score distribution across all evaluated patches.\n",
    "    \n",
    "    Args:\n",
    "        patch_metadata: List of dictionaries containing 'f1' scores for each patch\n",
    "        save_path: Optional path to save the figure\n",
    "    \"\"\"\n",
    "    f1_scores = [meta['f1'] for meta in patch_metadata]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    ax1 = axes[0]\n",
    "    \n",
    "    n, bins, patches_hist = ax1.hist(f1_scores, bins=30, color='steelblue', alpha=0.7, \n",
    "                                      edgecolor='black', linewidth=1.2)\n",
    "    \n",
    "    mean_f1 = np.mean(f1_scores)\n",
    "    median_f1 = np.median(f1_scores)\n",
    "    std_f1 = np.std(f1_scores)\n",
    "    min_f1 = np.min(f1_scores)\n",
    "    max_f1 = np.max(f1_scores)\n",
    "    \n",
    "    ax1.axvline(mean_f1, color='red', linestyle='--', linewidth=2.5, \n",
    "               label=f'Mean: {mean_f1:.4f}', alpha=0.8)\n",
    "    ax1.axvline(median_f1, color='orange', linestyle='--', linewidth=2.5, \n",
    "               label=f'Median: {median_f1:.4f}', alpha=0.8)\n",
    "    \n",
    "    ax1.axvspan(mean_f1 - std_f1, mean_f1 + std_f1, alpha=0.2, color='red', \n",
    "               label=f'±1 SD: [{mean_f1-std_f1:.4f}, {mean_f1+std_f1:.4f}]')\n",
    "    \n",
    "    ax1.set_xlabel('F1 Score', fontsize=13, fontweight='bold')\n",
    "    ax1.set_ylabel('Number of Patches', fontsize=13, fontweight='bold')\n",
    "    ax1.set_title(f'F1 Score Distribution\\n(n={len(f1_scores)} patches)', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax1.legend(loc='upper left', fontsize=11, framealpha=0.9)\n",
    "    ax1.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    stats_text = f\"\"\"Statistics:\n",
    "Mean:   {mean_f1:.4f}\n",
    "Median: {median_f1:.4f}\n",
    "Std:    {std_f1:.4f}\n",
    "Min:    {min_f1:.4f}\n",
    "Max:    {max_f1:.4f}\n",
    "\n",
    "Quartiles:\n",
    "Q1:     {np.percentile(f1_scores, 25):.4f}\n",
    "Q3:     {np.percentile(f1_scores, 75):.4f}\n",
    "IQR:    {np.percentile(f1_scores, 75) - np.percentile(f1_scores, 25):.4f}\"\"\"\n",
    "    \n",
    "    props = dict(boxstyle='round', facecolor='lightyellow', alpha=0.9, edgecolor='black', linewidth=1.5)\n",
    "    ax1.text(0.98, 0.97, stats_text, transform=ax1.transAxes, fontsize=10,\n",
    "            verticalalignment='top', horizontalalignment='right', bbox=props, fontfamily='monospace')\n",
    "    \n",
    "    ax2 = axes[1]\n",
    "    \n",
    "    parts = ax2.violinplot([f1_scores], positions=[1], widths=0.7, \n",
    "                           showmeans=True, showmedians=True, showextrema=True)\n",
    "    \n",
    "    # Customize violin plot colors\n",
    "    for pc in parts['bodies']:\n",
    "        pc.set_facecolor('steelblue')\n",
    "        pc.set_alpha(0.6)\n",
    "        pc.set_edgecolor('black')\n",
    "        pc.set_linewidth(1.5)\n",
    "    \n",
    "    for partname in ('cbars', 'cmins', 'cmaxes', 'cmedians', 'cmeans'):\n",
    "        if partname in parts:\n",
    "            vp = parts[partname]\n",
    "            vp.set_edgecolor('black')\n",
    "            vp.set_linewidth(2)\n",
    "    \n",
    "    bp = ax2.boxplot([f1_scores], positions=[1], widths=0.3, \n",
    "                     patch_artist=True, showfliers=True,\n",
    "                     boxprops=dict(facecolor='orange', alpha=0.7, edgecolor='black', linewidth=2),\n",
    "                     medianprops=dict(color='red', linewidth=2.5),\n",
    "                     whiskerprops=dict(color='black', linewidth=1.5),\n",
    "                     capprops=dict(color='black', linewidth=1.5),\n",
    "                     flierprops=dict(marker='o', markerfacecolor='red', markersize=6, \n",
    "                                   markeredgecolor='black', alpha=0.5))\n",
    "    \n",
    "    ax2.set_ylabel('F1 Score', fontsize=13, fontweight='bold')\n",
    "    ax2.set_title('F1 Score Distribution (Violin + Box Plot)', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xticks([1])\n",
    "    ax2.set_xticklabels(['All Patches'])\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    ax2.set_ylim([0, 1])\n",
    "    \n",
    "    q1 = np.percentile(f1_scores, 25)\n",
    "    q3 = np.percentile(f1_scores, 75)\n",
    "    ax2.axhline(mean_f1, color='red', linestyle='--', linewidth=1.5, alpha=0.5, label='Mean')\n",
    "    ax2.axhline(median_f1, color='orange', linestyle='--', linewidth=1.5, alpha=0.5, label='Median')\n",
    "    ax2.legend(loc='lower right', fontsize=10)\n",
    "    \n",
    "    ax2.text(1.4, median_f1, f'Median\\n{median_f1:.3f}', va='center', fontsize=9, fontweight='bold')\n",
    "    ax2.text(1.4, q1, f'Q1\\n{q1:.3f}', va='center', fontsize=9, color='darkblue')\n",
    "    ax2.text(1.4, q3, f'Q3\\n{q3:.3f}', va='center', fontsize=9, color='darkblue')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\" F1 distribution plot saved to {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary to console\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\" F1 SCORE DISTRIBUTION SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nTotal patches evaluated: {len(f1_scores)}\")\n",
    "    print(f\"\\nCentral Tendency:\")\n",
    "    print(f\"  Mean:   {mean_f1:.6f}\")\n",
    "    print(f\"  Median: {median_f1:.6f}\")\n",
    "    print(f\"\\nSpread:\")\n",
    "    print(f\"  Std Dev:  {std_f1:.6f}\")\n",
    "    print(f\"  Range:    [{min_f1:.6f}, {max_f1:.6f}]\")\n",
    "    print(f\"  IQR:      {np.percentile(f1_scores, 75) - np.percentile(f1_scores, 25):.6f}\")\n",
    "    print(f\"\\nQuartiles:\")\n",
    "    print(f\"  Q1 (25%): {np.percentile(f1_scores, 25):.6f}\")\n",
    "    print(f\"  Q2 (50%): {median_f1:.6f}\")\n",
    "    print(f\"  Q3 (75%): {np.percentile(f1_scores, 75):.6f}\")\n",
    "    print(f\"\\nPerformance Categories:\")\n",
    "    excellent = sum(1 for f1 in f1_scores if f1 >= 0.9)\n",
    "    good = sum(1 for f1 in f1_scores if 0.8 <= f1 < 0.9)\n",
    "    fair = sum(1 for f1 in f1_scores if 0.7 <= f1 < 0.8)\n",
    "    poor = sum(1 for f1 in f1_scores if f1 < 0.7)\n",
    "    print(f\"  Excellent (F1 ≥ 0.9): {excellent} ({excellent/len(f1_scores)*100:.1f}%)\")\n",
    "    print(f\"  Good (0.8 ≤ F1 < 0.9): {good} ({good/len(f1_scores)*100:.1f}%)\")\n",
    "    print(f\"  Fair (0.7 ≤ F1 < 0.8): {fair} ({fair/len(f1_scores)*100:.1f}%)\")\n",
    "    print(f\"  Poor (F1 < 0.7):       {poor} ({poor/len(f1_scores)*100:.1f}%)\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\" F1 distribution visualization function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d00673f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate F1 Score Distribution Plot\n",
    "\n",
    "print(\"Analyzing F1 score distribution across all evaluated patches...\")\n",
    "\n",
    "results = evaluate_and_select_examples(\n",
    "    min_keypoints=20,\n",
    "    n_candidates=10,\n",
    "    collect_correlation_metrics=False\n",
    ")\n",
    "\n",
    "best_candidates, average_candidates, worst_candidates = results\n",
    "\n",
    "print(\"\\nScanning all patches to collect F1 scores for distribution analysis...\")\n",
    "\n",
    "all_f1_metadata = []\n",
    "data_list = load_slide_data(DATA_ROOT)\n",
    "\n",
    "for i, data in enumerate(data_list):\n",
    "    if (i+1) % 100 == 0:\n",
    "        print(f\"  Progress: {i+1}/{len(data_list)}\")\n",
    "    \n",
    "    try:\n",
    "        img_raw = img_to_array(load_img(str(data['patch_path']), target_size=(512, 512)))\n",
    "        img_norm = img_raw / 255.0\n",
    "        cancer_mask, cancer_image = extract_cancer_region_ensemble(img_norm, tissue_model)\n",
    "        filtered_kps, _ = filter_keypoints_by_cancer_mask(data['patch_info']['keypoints'], cancer_mask)\n",
    "        \n",
    "        if len(filtered_kps) < 20:  # Skip patches with too few keypoints\n",
    "            continue\n",
    "        \n",
    "        if 'efficientnet' in BACKBONE:\n",
    "            mean = np.array([0.485, 0.456, 0.406])\n",
    "            std = np.array([0.229, 0.224, 0.225])\n",
    "            inp = (cancer_image - mean) / std\n",
    "        else:\n",
    "            inp = cancer_image\n",
    "        \n",
    "        pred = nuclei_model.predict(np.expand_dims(inp, axis=0), verbose=0)[0]\n",
    "        pred_mask = np.argmax(pred, axis=-1)\n",
    "        pred_mask_processed = apply_post_processing(pred_mask, min_object_size=50)\n",
    "        \n",
    "        f1, precision, recall, details = calculate_keypoint_coverage_f1(filtered_kps, pred_mask_processed)\n",
    "        \n",
    "        all_f1_metadata.append({\n",
    "            'f1': f1,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'filename': data['patch_info']['filename'],\n",
    "            'num_keypoints': len(filtered_kps)\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "print(f\"\\n Collected F1 scores from {len(all_f1_metadata)} patches\")\n",
    "\n",
    "plot_f1_distribution(all_f1_metadata)\n",
    "\n",
    "print(\"\\n F1 distribution analysis complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".wsl_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
